<!doctype html>
<html>

<head>
  <title>&#x2145;ial&#x2145;oc21 | HOME</title>
  <meta property="og:title" content="dialdoc workshop at ACL 2021" />
  <meta property="og:type" content="website" />
  <meta property="og:url" content="https://doc2dial.github.io/workshop2021/" />
  <meta property="og:image" content="https://doc2dial.github.io/workshop2021/file/dialdoc_banner.png" />
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">

  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <!-- Compiled and minified CSS -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/css/materialize.min.css">

  <!-- Compiled and minified JavaScript -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/materialize/1.0.0/js/materialize.min.js"></script>
  <script src="https://code.jquery.com/jquery-1.10.2.js"></script>


  <script>
    $(function () {
      $("#header").load("header.html");
    }); 
  </script>

</head>


<body class="blue-grey darken-2">
  <div id="header"></div>
  <main>
    <div class="container">
      <div class="card-panel white">
        <div class="section">
          <h4>Overview</h4>
          <p><b>Welcome to 1st DialDoc Workshop at ACL-IJCNLP 2021!</b></p>
          <p>
            Goal-oriented conversational systems can unlock a vast amount of information in the associated
            documents, in which written and visual content dominate the way that individuals and organizations choose
            to present their interests and knowledge to the world. We posit that one scalable way to create
            personalized conversational systems is to have them arise directly from such content. There are significant
            individual research threads that show promises in handling heterogeneous knowledge embedded in the
            documents,
            including
            (1) unstructured content such as text passages
            (<a href="https://stanfordnlp.github.io/coqa/">CoQA</a>,
            <a href="https://quac.ai/">QuAC</a>,
            <a href="https://sharc-data.github.io/">ShARC</a>,
            <a href="http://www.ixa.eus/node/12931">DoQA</a>);
            (2) semi-structured content such as tables or lists
            (<a href="http://aka.ms/sqa">SQA</a>,
            <a href="https://hybridqa.github.io/">HybridQA</a>);
            (3) multimedia such as images and videos with associated textual descriptions
            (<a href="https://hucvl.github.io/recipeqa/">RecipeQA</a>,
            <a href="https://sites.google.com/view/pstuts-vqa/home">PsTuts-VQA</a>);
            (4) or structured data specified by schema such as RDFa or Microdata in the webpages.
            The purpose of this workshop on Document-grounded Dialogue and Conversational QA is to invite researchers to
            bring their individual perspectives on the subject, and to advance the field in joint effort by
            providing a shared task and competition.
          </p>

        </div>

        <div class="section">
          <h4 id="cfp">Call for Papers</h4>
          <p class="text">
            We welcome submissions of original work as <i>long</i> and <i>short</i> papers.
            All accepted papers will be presented at the workshop.
          </p>
          <div>
            <h6><b>Topics of interest</b></h6>
            We encourage submissions from a wide range of topics, including but not limited to:
            <ul class="browser-default">
              <li class="collection-item">Document-grounded dialogue / conversational machine reading</li>
              <li class="collection-item">Machine reading comprehension / contextual QA / sequential QA over documents
              </li>
              <li class="collection-item">Conversational search among domain documents</li>
              <li class="collection-item">Topical chat based on associated documents</li>
              <li class="collection-item">Parsing semi-structured document content for sequential QA or dialogue / table
                reading</li>
              <li class="collection-item">Policy learning for document-guided dialogue agents</li>
              <li class="collection-item">Task-oriented knowledge-grounded multimodal dialogue
                <ul class="browser-default">
                  <li>task-oriented situated dialogue</li>
                  <li>task-oriented domain-knowledge-grounded VQA </li>
                  <li>task-oriented video-based dialogue</li>
                </ul>
              </li>
              <li class="collection-item">Evaluations for document-grounded dialogue</li>
              <li class="collection-item">Transfer learning for document-grounded dialogue systems on low-resource
                domains</li>
              <li class="collection-item">Summarization of document-grounded dialogues</li>
              <li class="collection-item">Model interpretation of document-grounded dialogues</li>
            </ul>
          </div>
          <h6><b>Submission Details</b></h6>
          <p style="margin-left: 30px"><i>Submission tracks</i>:<br>
            There will be two tracks: the <i>main research</i> track and <i>technical system</i> track.<br>
            The technical system track is mainly for paper submissions for <a href="shared.html">shared task</a>
          <blockquote style="margin-left: 30px">
            For main research track, please use <a href="https://www.softconf.com/acl2021/w11_dialdoc21/"
              target="_blank">Softconf
              link</a>.<br>
            For technical system track, please use <a href="https://www.softconf.com/acl2021/w11_dialdoc21_sharedtask/"
              target="_blank">Softconf link</a>.
          </blockquote>
          </p>
          <p style="margin-left: 30px"><i>Formatting Guidelines</i>:<br>
            We accept <i>long</i> (eight pages plus unlimited references) and <i>short</i> (four pages plus unlimited
            references) papers, which should
            conform to <a target="_blank" href="https://2021.aclweb.org/calls/papers/#paper-submission-information">ACL
              submission
              information</a>.
          </p>
          <p style="margin-left: 30px"><i>Non-Archival Submissions</i>: <br>
            The accepted papers can opt to be
            non-archival,
            i.e., the work could be
            published elsewhere before or after the workshop.</p>
          <div>
            <h6><b>Review Process</b></h6>
            <div style="margin-left: 30px">All submissions will be peer-reviewed by at least two reviewers. The
              reviewing
              process will be double-blinded at the level of the reviewers. Authors are responsible for anonymizing the
              submissions</div>
          </div>

          <!-- <h6><b>Submission Links</b></h6> -->
          <!-- <blockquote>
            For main research track, please use <a href="https://www.softconf.com/acl2021/w11_dialdoc21_sharedtask/"
              target="_blank">Softconf
              link</a>.<br>
            For technical system track, please use <a href="https://www.softconf.com/acl2021/w11_dialdoc21/"
              target="_blank">Softconf link</a>.
          </blockquote> -->

        </div>

        <div class="section">
          <h4>Important Dates</h4>
          <ul>
            <li>Workshop Paper Due Date: April 26, 2021 (AoE)</li>
            <li>Notification of Acceptance: May 28, 2021 (AoE)</li>
            <li>Camera-ready papers due: June 7, 2021 (AoE)</li>
            <li>Workshop Dates: August 5 or 6, 2021</li>
          </ul>
        </div>

        <div class="section">
          <h4 id="speakers">Invited Speakers</h4>
          <p>
            <a target="_blank" href="http://www.cs.tau.ac.il/~joberant/">Jonathan Berant</a> (Tel-Aviv University)<br>
            <a target="_blank" href="https://www.cs.princeton.edu/~danqic/">Danqi Chen</a> (Princeton University)<br>
            <a target="_blank" href="https://staff.fnwi.uva.nl/r.fernandezrovira/">Raquel Fern√°ndez</a> (University of
            Amsterdam)<br>
            <a target="_blank" href="https://www.linkedin.com/in/dilek-hakkani-tur-9517543/">Dilek Hakkani-Tur</a>
            (Amazon Alexa AI)<br>
            <a target="_blank" href="http://www.riedelcastro.org">Sebastian Riedel</a> (University College London)<br>
            <a target="_blank" href="https://sites.google.com/site/verenateresarieser/">Verena Rieser</a> (Heriot-Watt
            University)<br>
            <a target="_blank" href="http://www.thespermwhale.com/jaseweston/">Jason Weston</a> (Facebook AI
            Research)<br>
            <a target="_blank" href="https://sites.cs.ucsb.edu/~william/">William Wang Yang</a> (University of
            California, Santa Barbara)<br>
            <a target="_blank" href="http://scottyih.org">Scott (Wen-tau) Yih</a> (Facebook AI Research)
          </p>
        </div>

        <div class="section">
          <h4 id="pc">Program Committee</h4>
          <p>
            Amanda Buddemeyer (University of Pittsburgh)<br>
            Asli Celikyilmaz (Microsoft Research)<br>
            Chengguang Tang (Alibaba DAMO) <br>
            Chulaka Gunasekara (IBM Research AI)<br>
            Dian Yu (Tencent)<br>
            Diane Litman (University of Pittsburgh)<br>
            Ehud Reiter (University of Aberdeen)<br>
            Elizabeth Clark (University of Washington)<br>
            Eunsol Choi (University of Texas at Austin)<br>
            Hanjie Chen (University of Virginia)<br>
            Hui Wan (IBM Research AI)<br>
            Ioannis Konstas (Heriot-Watt University)<br>
            Jiwei Li (SHANNON.AI)<br>
            Michael Johnston (Interactions)<br>
            Minjoon Seo (KAIST)<br>
            Peng Qi (Stanford))<br>
            Ravneet Singh (University of Pittsburgh)<br>
            Seokhwan Kim (Amazon Alexa AI)<br>
            Shehzaad Dhuliawala (Microsoft Research Montreal)<br>
            Srinivas Bangalore (Interactions)<br>
            Vaibhav Adlakha (McGill and Mila)<br>
            Xiaoxiao Guo (IBM Research AI)<br>
          </p>
        </div>

        <div class="section">
          <h4 id="organizers">Organizers</h4>
          <p class="text">
            <a target="_blank" href="https://www.malihealikhani.com">Malihe Alikhani</a> (University of Pittsburgh)<br>
            <a target="_blank" href="https://researcher.watson.ibm.com/researcher/view.php?person=us-sfeng">Song
              Feng</a> (IBM Research)<br>
            <a target="_blank" href="https://hhexiy.github.io">He He</a> (New York University)<br>
            <a target="_blank" href="https://people.cs.umass.edu/~miyyer/">Mohit Iyyer</a> (University of Massachusetts
            Amherst)<br>
            <a target="_blank" href="https://yangfengji.net">Yangfeng Ji</a> (University of Virginia)<br>
            <a target="_blank" href="https://sivareddy.in">Siva Reddy</a> (McGill University, MILA)<br>
            <a target="_blank" href="http://zhouyu.cs.ucdavis.edu">Zhou Yu</a> (Columbia University)<br>
          </p>
        </div>
      </div>
    </div>
  </main>
</body>

</html>